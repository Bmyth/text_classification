{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取news20训练数据集\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "    \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), data_home='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "[ 7  4  4  1 14 16 13  3  2  4]\n"
     ]
    }
   ],
   "source": [
    "# 样本数量 & target分类结果为 1 - 20\n",
    "print(len(newsgroups_train.data))\n",
    "print(newsgroups_train.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was wondering if anyone out there could enlighten me on this car i saw the other day it was a 2 door sports car looked to be from the late 60s early 70s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail\n"
     ]
    }
   ],
   "source": [
    "# text 预处理\n",
    "# 去除符号 字母小写\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "data = []\n",
    "for text in newsgroups_train.data:\n",
    "    data.append(\" \".join(tokenizer.tokenize(text.lower())))\n",
    "    \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 39551\n"
     ]
    }
   ],
   "source": [
    "# text 预处理\n",
    "# 构建字典 将词专为index\n",
    "# 然后截取固定长度作为输入 x\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "max_document_length = 1000\n",
    "min_frequency = 2\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, min_frequency)\n",
    "x = np.array(list(vocab_processor.fit_transform(newsgroups_train.data)))\n",
    "\n",
    "print(\"vocabulary size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    7    26  1354    42   177    62    51   108 13015    57    14    20\n",
      "   330     7   610     1    75   301    78    26     5 32125  3220   330\n",
      "   963     3    15    30     1  1256 15054   799 15061    78    26   296\n",
      "     5     0    21  4103    67   182   428   109  1078     1   852  9115\n",
      "    26  1649    30     1   680     4     1   673    93     8    44     7\n",
      "    83    73   177    40     0     5   929   283  1157  3516   183     4\n",
      "  2911   159    20   330     8   209   675    25   784   422    13    16\n",
      "    14    20 25542   371   330   311   663     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print((x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  4  4  1 14 16 13  3  2  4  8 19  4 14  6  0  1  7 12  5  0 10  6  2  4\n",
      "  1 12  9 15  7  6 13 12 17 18 10  8 11  8 16]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# 分类结果 one-hot 处理\n",
    "\n",
    "def one_hot_encode(x, length):\n",
    "    encode_labels = np.zeros(shape=(len(x),length))\n",
    "   \n",
    "    for i in range(len(x)):\n",
    "        label = x[i]\n",
    "        a = np.zeros(length)\n",
    "        a[label] = 1\n",
    "        encode_labels[i] = a\n",
    "    return encode_labels\n",
    "y = one_hot_encode(newsgroups_train.target, len(newsgroups_train.target_names))\n",
    "print(newsgroups_train.target[0:40])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data shuffle\n",
    "\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(newsgroups_train.target)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test split: 10183/1131\n"
     ]
    }
   ],
   "source": [
    "# 训练样本/测试样本 split\n",
    "\n",
    "sample_rate = .1\n",
    "dev_sample_index = -1 * int(sample_rate * float(len(newsgroups_train.target)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "print(\"train/test split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text CNN\n",
    "# x = word2vec(x) \n",
    "# x[n] = maxpooling(CNN(x)), n = filter size (2,3,4)\n",
    "# x = dropout(combine(x[n]))\n",
    "# output = fullyconnect(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "def TextCNN(input_x, dropout_keep_prob, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters):\n",
    "    \n",
    "    # Embedding layer\n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "        w0 = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "        embedded_chars = tf.nn.embedding_lookup(w0, input_x)\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        W = tf.Variable(tf.random_normal([num_filters_total, num_classes], mean=0.0, stddev = 0.2))\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "        logits = tf.nn.xw_plus_b(h_drop, W, b, name=\"logits\")\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text RNN\n",
    "# x = word2vec(x) \n",
    "# x = last output of LSTM(x), layer = 1\n",
    "# x = dropout(fullyconnect(x)) \n",
    "# output = fullyconnect(x)\n",
    "\n",
    "num_layers = 1\n",
    "def TextRNN(input_x, dropout_keep_prob, sequence_length, num_classes, vocab_size, embedding_size, rnnsize, fcsize):\n",
    "    # Embedding layers\n",
    "    w0 = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "    embedded_chars = tf.nn.embedding_lookup(w0, input_x)\n",
    "    \n",
    "     batch_size = tf.shape(outputs)[0]\n",
    "        \n",
    "#         lstm_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnnsize)] * num_layers), output_keep_prob=self.dropout_keep_prob)\n",
    "    lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnnsize)] * num_layers)\n",
    "    initial_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, embedded_chars, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    index = tf.range(0, batch_size) * sequence_length + (sequence_length - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, rnnsize]), index)\n",
    "\n",
    "    fc = tf.contrib.layers.fully_connected(outputs, fcsize, weights_initializer = tf.truncated_normal_initializer(stddev = 0.01), activation_fn=None)\n",
    "    flat = tf.nn.dropout(fc, tf.to_float(dropout_keep_prob))\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    w = tf.Variable(tf.random_normal([fcsize, num_classes], mean=0.0, stddev=0.1))\n",
    "    b = tf.Variable(tf.random_normal([num_classes], mean=0.0, stddev=0.1))\n",
    "    logits = tf.nn.xw_plus_b(flat, w, b, name=\"scores\")\n",
    "    return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch method\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "embedding_dim = 80\n",
    "filter_sizes = \"3,4,5\"\n",
    "num_filters = 400\n",
    "dropout_keep_prob = 0.8\n",
    "rnnsize = 500\n",
    "fcsize = 400\n",
    "batch_size = 90\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "evaluate_every = 20\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "sequence_length=x_train.shape[1]\n",
    "vocabulary_len = len(vocab_processor.vocabulary_)\n",
    "\n",
    "# build the network graph\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    lr = tf.placeholder(tf.float32, name='LearingRate')\n",
    "    dropout_keep = tf.placeholder(tf.float32, name='dropout')\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int32, [None, num_classes], name=\"input_y\")\n",
    "    \n",
    "\n",
    "#     logits = TextRNN(\n",
    "#         input_x,\n",
    "#         dropout_keep,\n",
    "#         sequence_length,\n",
    "#         num_classes,\n",
    "#         vocabulary_len,\n",
    "#         embedding_dim,\n",
    "#         rnnsize,\n",
    "#         fcsize)\n",
    "    \n",
    "    logits = TextCNN(\n",
    "        input_x,\n",
    "        dropout_keep,\n",
    "        sequence_length, \n",
    "        num_classes, \n",
    "        vocabulary_len, \n",
    "        embedding_dim, \n",
    "        [int(s) for s in filter_sizes.split(',')], \n",
    "        fcsize)\n",
    "    \n",
    "    logits = tf.identity(logits, name='logits')\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    index_predict = tf.argmax(logits, 1)\n",
    "    index_target = tf.argmax(input_y, 1)\n",
    "    correct_pred = tf.equal(index_predict, index_target)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy') \n",
    "    \n",
    "    def train_step(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "          input_x: x_batch,\n",
    "          input_y: y_batch,\n",
    "          dropout_keep: dropout_keep_prob,\n",
    "          lr: learning_rate\n",
    "        }\n",
    "        _, _loss, _accuracy = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}:, loss {:g}, acc {:g}\".format(time_str, _loss, _accuracy))\n",
    "\n",
    "    def dev_step(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "          input_x: x_batch,\n",
    "          input_y: y_batch,\n",
    "          dropout_keep: 1.0\n",
    "        }\n",
    "        _loss, _accuracy = sess.run(\n",
    "            [cost, accuracy],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"loss {:g}, acc {:g}\".format(_loss, _accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1\n",
      "2017-07-20T16:47:34.630392:, loss 31.5443, acc 0.0444444\n",
      "batch 2\n",
      "2017-07-20T16:47:43.659439:, loss 32.6976, acc 0.0111111\n",
      "batch 3\n",
      "2017-07-20T16:47:53.944320:, loss 27.7159, acc 0.0777778\n",
      "batch 4\n",
      "2017-07-20T16:48:05.548959:, loss 24.3772, acc 0.0666667\n",
      "batch 5\n",
      "2017-07-20T16:48:14.578692:, loss 20.3478, acc 0.0444444\n",
      "batch 6\n",
      "2017-07-20T16:48:24.633118:, loss 19.7353, acc 0.0888889\n",
      "batch 7\n",
      "2017-07-20T16:48:36.352789:, loss 17.175, acc 0.1\n",
      "batch 8\n",
      "2017-07-20T16:48:46.419663:, loss 17.618, acc 0.0333333\n",
      "batch 9\n",
      "2017-07-20T16:48:57.665482:, loss 17.568, acc 0.0777778\n",
      "batch 10\n",
      "2017-07-20T16:49:07.255401:, loss 18.7636, acc 0.0333333\n",
      "batch 11\n",
      "2017-07-20T16:49:16.053134:, loss 18.8175, acc 0.0555556\n",
      "batch 12\n",
      "2017-07-20T16:49:25.193652:, loss 17.9458, acc 0.0888889\n",
      "batch 13\n",
      "2017-07-20T16:49:34.314766:, loss 19.3747, acc 0.0222222\n",
      "batch 14\n",
      "2017-07-20T16:49:44.812599:, loss 19.6929, acc 0.0333333\n",
      "batch 15\n",
      "2017-07-20T16:49:55.040357:, loss 17.2731, acc 0.0888889\n",
      "batch 16\n",
      "2017-07-20T16:50:04.452123:, loss 18.4874, acc 0.0111111\n",
      "batch 17\n",
      "2017-07-20T16:50:13.725098:, loss 17.952, acc 0.0222222\n",
      "batch 18\n",
      "2017-07-20T16:50:23.203856:, loss 16.4964, acc 0.0666667\n",
      "batch 19\n",
      "2017-07-20T16:50:34.332036:, loss 17.1463, acc 0.0333333\n",
      "batch 20\n",
      "2017-07-20T16:50:44.364325:, loss 16.2616, acc 0.0666667\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "import datetime\n",
    "    \n",
    "batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Generate batches\n",
    "    # Training loop. For each batch...\n",
    "    step = 1\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        \n",
    "        print(\"batch {}\".format(step))\n",
    "        train_step(x_batch, y_batch)\n",
    "\n",
    "        if step % evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            dev_step(x_dev, y_dev)\n",
    "            print(\"\")\n",
    "        step = step + 1\n",
    "        \n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "    save_model_path = './trained_model'\n",
    "    path = saver.save(sess, save_model_path)\n",
    "    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
